#!/usr/bin/env python3
"""
üß† ENGAGEMENT DATA ENRICHMENT - LAYER 2
Apply Universal Pipeline Intelligence to 17,841 real Instagram comments

Features:
- Enhanced sentiment analysis with confidence scoring
- Safety detection and content moderation
- Product intelligence and mention extraction
- Intent classification (praise, complaint, question, etc.)
- Temporal pattern analysis
- User behavior profiling
- UGC opportunity identification
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sqlite3
import json
from pathlib import Path

# Import our intelligence modules
from universal_pipeline import UniversalPipeline
from enhanced_sentiment import analyze_comment_enhanced
from safety_detection import analyze_comment_safety_comprehensive
from product_intelligence import ProductPerformanceAnalyzer
from multi_modal_bertopic import analyze_comments_multimodal

class EngagementEnrichment:
    """Layer 2: Enrich real engagement data with AI intelligence"""
    
    def __init__(self, csv_path='../engagements.csv', db_path='data/engagement_intelligence.db'):
        """Initialize enrichment pipeline"""
        print("üöÄ Initializing Engagement Intelligence Pipeline...")
        
        self.csv_path = csv_path
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True)
        
        # Load raw data
        self.df = pd.read_csv(csv_path)
        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], format='mixed')
        
        # Initialize intelligence modules
        self.universal_pipeline = UniversalPipeline()
        self.product_analyzer = ProductPerformanceAnalyzer()
        
        print(f"‚úÖ Loaded {len(self.df):,} comments for intelligence processing")
        
    def create_enriched_schema(self):
        """Create SQLite schema for enriched engagement data"""
        print("üìä Creating enriched database schema...")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Main enriched comments table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS enriched_comments (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    original_timestamp TEXT NOT NULL,
                    media_id INTEGER NOT NULL,
                    media_caption TEXT,
                    comment_text TEXT,
                    
                    -- Basic metrics
                    comment_length INTEGER,
                    word_count INTEGER,
                    
                    -- Enhanced sentiment analysis
                    sentiment_label TEXT,
                    sentiment_score REAL,
                    sentiment_confidence REAL,
                    emoji_influence REAL,
                    
                    -- Intent classification
                    intent_label TEXT,
                    intent_confidence REAL,
                    intent_secondary TEXT,
                    
                    -- Safety detection
                    safety_detected BOOLEAN DEFAULT FALSE,
                    safety_severity TEXT DEFAULT 'none',
                    safety_score REAL DEFAULT 0.0,
                    safety_categories TEXT DEFAULT '[]',
                    
                    -- Product intelligence
                    product_mentions TEXT DEFAULT '[]',
                    product_sentiment TEXT DEFAULT '{}',
                    brand_mentions TEXT DEFAULT '[]',
                    
                    -- Temporal intelligence
                    hour_of_day INTEGER,
                    day_of_week TEXT,
                    week_of_year INTEGER,
                    is_peak_hour BOOLEAN DEFAULT FALSE,
                    
                    -- Engagement intelligence
                    has_emoji BOOLEAN DEFAULT FALSE,
                    has_mention BOOLEAN DEFAULT FALSE,
                    has_hashtag BOOLEAN DEFAULT FALSE,
                    has_question BOOLEAN DEFAULT FALSE,
                    is_substantive BOOLEAN DEFAULT FALSE,
                    
                    -- UGC potential
                    ugc_score REAL DEFAULT 0.0,
                    ugc_category TEXT DEFAULT 'none',
                    ugc_approved BOOLEAN DEFAULT FALSE,
                    
                    -- Processing metadata
                    processing_version TEXT DEFAULT 'v2.0',
                    enriched_at TEXT,
                    quality_score REAL DEFAULT 0.0
                )
            """)
            
            # Post performance summary table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS post_intelligence (
                    media_id INTEGER PRIMARY KEY,
                    media_caption TEXT,
                    
                    -- Engagement metrics
                    total_comments INTEGER DEFAULT 0,
                    avg_sentiment REAL DEFAULT 0.0,
                    sentiment_distribution TEXT DEFAULT '{}',
                    
                    -- Safety metrics
                    safety_issues INTEGER DEFAULT 0,
                    safety_severity_max TEXT DEFAULT 'none',
                    
                    -- Product performance
                    product_mentions TEXT DEFAULT '[]',
                    top_products TEXT DEFAULT '[]',
                    
                    -- UGC potential
                    ugc_candidates INTEGER DEFAULT 0,
                    high_quality_comments INTEGER DEFAULT 0,
                    
                    -- Temporal insights
                    peak_engagement_hour INTEGER,
                    engagement_pattern TEXT DEFAULT '{}',
                    
                    -- Business intelligence
                    business_priority TEXT DEFAULT 'medium',
                    recommended_actions TEXT DEFAULT '[]',
                    
                    last_analyzed TEXT
                )
            """)
            
            # Temporal insights table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS temporal_insights (
                    date TEXT PRIMARY KEY,
                    total_comments INTEGER DEFAULT 0,
                    avg_sentiment REAL DEFAULT 0.0,
                    safety_issues INTEGER DEFAULT 0,
                    product_mentions INTEGER DEFAULT 0,
                    ugc_opportunities INTEGER DEFAULT 0,
                    peak_hour INTEGER,
                    engagement_quality REAL DEFAULT 0.0,
                    business_health_score REAL DEFAULT 0.0
                )
            """)
            
            # Create indexes for performance
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_media_id ON enriched_comments(media_id)",
                "CREATE INDEX IF NOT EXISTS idx_timestamp ON enriched_comments(original_timestamp)",
                "CREATE INDEX IF NOT EXISTS idx_sentiment ON enriched_comments(sentiment_label, sentiment_score)",
                "CREATE INDEX IF NOT EXISTS idx_safety ON enriched_comments(safety_detected, safety_severity)",
                "CREATE INDEX IF NOT EXISTS idx_intent ON enriched_comments(intent_label)",
                "CREATE INDEX IF NOT EXISTS idx_ugc ON enriched_comments(ugc_score, ugc_category)"
            ]
            
            for index_sql in indexes:
                cursor.execute(index_sql)
            
            conn.commit()
            
        print("‚úÖ Database schema created with intelligence-ready structure")
    
    def enrich_batch(self, batch_df, batch_num, total_batches):
        """Enrich a batch of comments with AI intelligence"""
        enriched_batch = []
        
        print(f"üß† Processing batch {batch_num}/{total_batches} ({len(batch_df)} comments)...")
        
        for idx, row in batch_df.iterrows():
            try:
                comment_text = str(row['comment_text']) if pd.notna(row['comment_text']) else ""
                
                # Skip empty comments
                if len(comment_text.strip()) < 2:
                    continue
                
                enriched_comment = {
                    'original_timestamp': row['timestamp'].isoformat(),
                    'media_id': int(row['media_id']),
                    'media_caption': str(row['media_caption']) if pd.notna(row['media_caption']) else "",
                    'comment_text': comment_text,
                    'comment_length': len(comment_text),
                    'word_count': len(comment_text.split())
                }
                
                # 1. Enhanced Sentiment Analysis
                try:
                    sentiment_result = analyze_comment_enhanced(comment_text)
                    enriched_comment.update({
                        'sentiment_label': sentiment_result['sentiment_label'],
                        'sentiment_score': float(sentiment_result['sentiment_score']),
                        'sentiment_confidence': float(sentiment_result['sentiment_confidence']),
                        'emoji_influence': float(sentiment_result.get('emoji_influence', 0.0))
                    })
                except Exception as e:
                    print(f"Sentiment analysis failed for comment {idx}: {e}")
                    enriched_comment.update({
                        'sentiment_label': 'neutral',
                        'sentiment_score': 0.0,
                        'sentiment_confidence': 0.5,
                        'emoji_influence': 0.0
                    })
                
                # 2. Intent Classification
                try:
                    # Simple intent classification based on patterns
                    intent_result = self._classify_intent(comment_text)
                    enriched_comment.update({
                        'intent_label': intent_result['primary'],
                        'intent_confidence': intent_result['confidence'],
                        'intent_secondary': intent_result.get('secondary', '')
                    })
                except Exception as e:
                    enriched_comment.update({
                        'intent_label': 'general',
                        'intent_confidence': 0.5,
                        'intent_secondary': ''
                    })
                
                # 3. Safety Detection
                try:
                    safety_result = analyze_comment_safety_comprehensive(comment_text)
                    enriched_comment.update({
                        'safety_detected': bool(safety_result['safety_detected']),
                        'safety_severity': safety_result['severity'],
                        'safety_score': float(safety_result['safety_score']),
                        'safety_categories': json.dumps(safety_result['safety_categories'])
                    })
                except Exception as e:
                    enriched_comment.update({
                        'safety_detected': False,
                        'safety_severity': 'none',
                        'safety_score': 0.0,
                        'safety_categories': '[]'
                    })
                
                # 4. Product Intelligence
                try:
                    product_result = self._extract_product_mentions(comment_text)
                    enriched_comment.update({
                        'product_mentions': json.dumps(product_result['products']),
                        'product_sentiment': json.dumps(product_result['sentiment']),
                        'brand_mentions': json.dumps(product_result['brands'])
                    })
                except Exception as e:
                    enriched_comment.update({
                        'product_mentions': '[]',
                        'product_sentiment': '{}',
                        'brand_mentions': '[]'
                    })
                
                # 5. Temporal Intelligence
                timestamp = pd.to_datetime(row['timestamp'])
                enriched_comment.update({
                    'hour_of_day': timestamp.hour,
                    'day_of_week': timestamp.day_name(),
                    'week_of_year': timestamp.isocalendar().week,
                    'is_peak_hour': timestamp.hour in [18, 19, 20, 21]  # Based on EDA findings
                })
                
                # 6. Engagement Features
                enriched_comment.update({
                    'has_emoji': self._has_emoji(comment_text),
                    'has_mention': '@' in comment_text,
                    'has_hashtag': '#' in comment_text,
                    'has_question': '?' in comment_text,
                    'is_substantive': len(comment_text) >= 20
                })
                
                # 7. UGC Potential Scoring
                ugc_result = self._calculate_ugc_score(enriched_comment)
                enriched_comment.update({
                    'ugc_score': ugc_result['score'],
                    'ugc_category': ugc_result['category'],
                    'ugc_approved': False  # Manual approval required
                })
                
                # 8. Quality Score
                enriched_comment['quality_score'] = self._calculate_quality_score(enriched_comment)
                enriched_comment['enriched_at'] = datetime.now().isoformat()
                
                enriched_batch.append(enriched_comment)
                
            except Exception as e:
                print(f"Failed to enrich comment {idx}: {e}")
                continue
        
        return enriched_batch
    
    def _classify_intent(self, text):
        """Simple intent classification"""
        text_lower = text.lower()
        
        # Define intent patterns
        patterns = {
            'praise': ['love', 'amazing', 'great', 'perfect', 'best', 'awesome', 'beautiful', 'fantastic'],
            'complaint': ['hate', 'bad', 'terrible', 'awful', 'worst', 'disappointed', 'horrible', 'disgusting'],
            'question': ['?', 'how', 'what', 'when', 'where', 'why', 'which'],
            'request': ['please', 'can you', 'could you', 'would you', 'need', 'want'],
            'information': ['where to buy', 'available', 'store', 'price', 'cost', 'ingredients']
        }
        
        scores = {}
        for intent, keywords in patterns.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                scores[intent] = score / len(keywords)
        
        if scores:
            primary_intent = max(scores, key=scores.get)
            confidence = min(1.0, scores[primary_intent] * 2)  # Scale confidence
            
            # Get secondary intent
            remaining_scores = {k: v for k, v in scores.items() if k != primary_intent}
            secondary_intent = max(remaining_scores, key=remaining_scores.get) if remaining_scores else ""
            
            return {
                'primary': primary_intent,
                'confidence': confidence,
                'secondary': secondary_intent
            }
        
        return {'primary': 'general', 'confidence': 0.5, 'secondary': ''}
    
    def _extract_product_mentions(self, text):
        """Extract product and brand mentions"""
        text_lower = text.lower()
        
        # Tree Hut product patterns (from EDA)
        products = {
            'Moroccan Rose': ['moroccan rose', 'moroccan', 'rose'],
            'Espresso Martini': ['espresso martini', 'espresso', 'martini'],
            'Coconut Lime': ['coconut lime', 'coconut', 'lime'],
            'Vanilla Dream': ['vanilla dream', 'vanilla'],
            'Tropical Mist': ['tropical mist', 'tropical'],
            'Shea Sugar Scrub': ['shea sugar scrub', 'shea scrub', 'sugar scrub'],
            'Brazilian Bum Bum': ['brazilian bum', 'bum bum'],
            'Honey Oat': ['honey oat', 'honey']
        }
        
        brands = {
            'Tree Hut': ['tree hut', 'treehut'],
            'Ulta': ['ulta', 'ulta beauty'],
            'Sephora': ['sephora'],
            'Target': ['target']
        }
        
        found_products = []
        found_brands = []
        product_sentiments = {}
        
        # Check for product mentions
        for product, patterns in products.items():
            for pattern in patterns:
                if pattern in text_lower:
                    found_products.append(product)
                    # Simple sentiment for this mention (could be enhanced)
                    if any(pos in text_lower for pos in ['love', 'great', 'amazing', 'perfect', 'best']):
                        product_sentiments[product] = 'positive'
                    elif any(neg in text_lower for neg in ['hate', 'bad', 'terrible', 'awful']):
                        product_sentiments[product] = 'negative'
                    else:
                        product_sentiments[product] = 'neutral'
                    break
        
        # Check for brand mentions
        for brand, patterns in brands.items():
            for pattern in patterns:
                if pattern in text_lower:
                    found_brands.append(brand)
                    break
        
        return {
            'products': list(set(found_products)),
            'brands': list(set(found_brands)),
            'sentiment': product_sentiments
        }
    
    def _has_emoji(self, text):
        """Check if text contains emojis"""
        try:
            import emoji
            return any(emoji.is_emoji(char) for char in str(text))
        except:
            return False
    
    def _calculate_ugc_score(self, enriched_comment):
        """Calculate UGC potential score"""
        score = 0.0
        category = 'none'
        
        # Positive factors
        if enriched_comment['sentiment_label'] == 'positive':
            score += 0.3
        if enriched_comment['is_substantive']:
            score += 0.2
        if enriched_comment['has_emoji']:
            score += 0.1
        if enriched_comment['intent_label'] == 'praise':
            score += 0.2
        if len(enriched_comment.get('product_mentions', '[]')) > 2:  # JSON string length check
            score += 0.1
        if not enriched_comment['safety_detected']:
            score += 0.1
        
        # Determine category
        if score >= 0.7:
            category = 'high_potential'
        elif score >= 0.5:
            category = 'medium_potential'
        elif score >= 0.3:
            category = 'low_potential'
        else:
            category = 'none'
        
        return {'score': round(score, 3), 'category': category}
    
    def _calculate_quality_score(self, enriched_comment):
        """Calculate overall comment quality score"""
        factors = [
            enriched_comment['sentiment_confidence'],
            enriched_comment['intent_confidence'],
            1.0 if enriched_comment['is_substantive'] else 0.5,
            1.0 if not enriched_comment['safety_detected'] else 0.0,
            min(1.0, enriched_comment['comment_length'] / 50.0)  # Length factor
        ]
        
        return round(sum(factors) / len(factors), 3)
    
    def run_enrichment(self, batch_size=1000):
        """Run complete enrichment process"""
        print("\n" + "="*80)
        print("üß† LAYER 2: ENGAGEMENT DATA ENRICHMENT")
        print("="*80)
        
        # Create database schema
        self.create_enriched_schema()
        
        # Process in batches for memory efficiency
        total_comments = len(self.df)
        batches = [self.df[i:i+batch_size] for i in range(0, total_comments, batch_size)]
        total_batches = len(batches)
        
        print(f"üìä Processing {total_comments:,} comments in {total_batches} batches...")
        
        all_enriched = []
        
        for batch_num, batch_df in enumerate(batches, 1):
            enriched_batch = self.enrich_batch(batch_df, batch_num, total_batches)
            all_enriched.extend(enriched_batch)
            
            # Save batch to database
            if enriched_batch:
                self._save_batch_to_db(enriched_batch)
            
            print(f"‚úÖ Batch {batch_num}/{total_batches} complete ({len(enriched_batch)} enriched)")
        
        # Generate post-level intelligence
        self._generate_post_intelligence()
        
        # Generate temporal insights
        self._generate_temporal_insights()
        
        print(f"\nüéâ Enrichment Complete!")
        print(f"   üìä Processed: {len(all_enriched):,} comments")
        print(f"   üíæ Saved to: {self.db_path}")
        print(f"   üß† Intelligence layers applied: Sentiment, Safety, Products, Intent, UGC")
        
        return all_enriched
    
    def _save_batch_to_db(self, enriched_batch):
        """Save enriched batch to database"""
        with sqlite3.connect(self.db_path) as conn:
            for comment in enriched_batch:
                placeholders = ', '.join(['?' for _ in comment.keys()])
                columns = ', '.join(comment.keys())
                
                conn.execute(
                    f"INSERT INTO enriched_comments ({columns}) VALUES ({placeholders})",
                    list(comment.values())
                )
            conn.commit()
    
    def _generate_post_intelligence(self):
        """Generate post-level intelligence summary"""
        print("üìä Generating post-level intelligence...")
        
        with sqlite3.connect(self.db_path) as conn:
            # Get post-level aggregations
            query = """
                SELECT 
                    media_id,
                    media_caption,
                    COUNT(*) as total_comments,
                    AVG(sentiment_score) as avg_sentiment,
                    SUM(CASE WHEN safety_detected THEN 1 ELSE 0 END) as safety_issues,
                    SUM(CASE WHEN ugc_score >= 0.5 THEN 1 ELSE 0 END) as ugc_candidates,
                    SUM(CASE WHEN is_substantive THEN 1 ELSE 0 END) as high_quality_comments,
                    AVG(quality_score) as avg_quality
                FROM enriched_comments 
                GROUP BY media_id, media_caption
            """
            
            post_stats = pd.read_sql_query(query, conn)
            
            # Generate business priorities and recommendations for each post
            for _, post in post_stats.iterrows():
                business_priority = 'medium'
                recommendations = []
                
                if post['safety_issues'] > 0:
                    business_priority = 'high'
                    recommendations.append('Review safety issues')
                
                if post['avg_sentiment'] < 0.3:
                    business_priority = 'high'
                    recommendations.append('Address negative sentiment')
                
                if post['ugc_candidates'] > 5:
                    recommendations.append('Review UGC opportunities')
                
                if post['total_comments'] > 100:
                    recommendations.append('High engagement - amplify content')
                
                # Insert post intelligence
                conn.execute("""
                    INSERT OR REPLACE INTO post_intelligence 
                    (media_id, media_caption, total_comments, avg_sentiment, 
                     safety_issues, ugc_candidates, high_quality_comments, 
                     business_priority, recommended_actions, last_analyzed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    post['media_id'], post['media_caption'], post['total_comments'],
                    post['avg_sentiment'], post['safety_issues'], post['ugc_candidates'],
                    post['high_quality_comments'], business_priority,
                    json.dumps(recommendations), datetime.now().isoformat()
                ))
            
            conn.commit()
    
    def _generate_temporal_insights(self):
        """Generate daily temporal insights"""
        print("‚è∞ Generating temporal insights...")
        
        with sqlite3.connect(self.db_path) as conn:
            query = """
                SELECT 
                    DATE(original_timestamp) as date,
                    COUNT(*) as total_comments,
                    AVG(sentiment_score) as avg_sentiment,
                    SUM(CASE WHEN safety_detected THEN 1 ELSE 0 END) as safety_issues,
                    SUM(CASE WHEN product_mentions != '[]' THEN 1 ELSE 0 END) as product_mentions,
                    SUM(CASE WHEN ugc_score >= 0.5 THEN 1 ELSE 0 END) as ugc_opportunities,
                    AVG(quality_score) as engagement_quality
                FROM enriched_comments 
                GROUP BY DATE(original_timestamp)
                ORDER BY date
            """
            
            daily_stats = pd.read_sql_query(query, conn)
            
            for _, day in daily_stats.iterrows():
                # Calculate business health score
                health_factors = [
                    min(1.0, day['avg_sentiment']),
                    1.0 - (day['safety_issues'] / max(day['total_comments'], 1)),
                    min(1.0, day['engagement_quality']),
                    min(1.0, day['product_mentions'] / max(day['total_comments'], 1) * 10)
                ]
                
                health_score = sum(health_factors) / len(health_factors) * 100
                
                conn.execute("""
                    INSERT OR REPLACE INTO temporal_insights 
                    (date, total_comments, avg_sentiment, safety_issues, 
                     product_mentions, ugc_opportunities, engagement_quality, business_health_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    day['date'], day['total_comments'], day['avg_sentiment'],
                    day['safety_issues'], day['product_mentions'], day['ugc_opportunities'],
                    day['engagement_quality'], health_score
                ))
            
            conn.commit()

def main():
    """Run Layer 2 enrichment"""
    try:
        enricher = EngagementEnrichment()
        enriched_data = enricher.run_enrichment(batch_size=500)  # Smaller batches for stability
        
        print("\nüöÄ READY FOR LAYER 3: Insights Dashboard & Business Intelligence")
        return enriched_data
        
    except Exception as e:
        print(f"‚ùå Enrichment failed: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main() 